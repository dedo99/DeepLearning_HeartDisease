{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edc8066",
   "metadata": {},
   "source": [
    "# Addestramento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.legacy_tf_layers.convolutional import Conv1D\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "!pip install d2l==1.0.0a1.post0\n",
    "from d2l import tensorflow as d2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"valid\", input_shape=[300, 1]),\n",
    "keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"valid\"),\n",
    "keras.layers.MaxPooling1D(pool_size=2, strides=2),\n",
    "keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"valid\"),\n",
    "keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"valid\"),\n",
    "keras.layers.MaxPooling1D(pool_size=2, strides=2),\n",
    "keras.layers.Flatten(),\n",
    "keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=x_train, y=y_train, epochs=100)\n",
    "\n",
    "test_error_rate = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfbef5",
   "metadata": {},
   "source": [
    "# Addestramento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "input_tot = Input(shape=(251, 1), name =\"Input_tot\")\n",
    "\n",
    "# Pipeline 1\n",
    "branch1_1 = keras.layers.Conv1D(filters=8, kernel_size=4, activation='relu', name =\"branch1_1\")(input_tot)\n",
    "branch1_2 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch1_2\")(branch1_1)\n",
    "branch1_3 = keras.layers.Conv1D(filters=24, kernel_size=6, activation='relu', name =\"branch1_3\")(branch1_2)\n",
    "branch1_4 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch1_4\")(branch1_3)\n",
    "\n",
    "# Pipeline 2\n",
    "branch2_1 = keras.layers.Conv1D(filters=8, kernel_size=6, activation='relu', name =\"branch2_1\")(input_tot)\n",
    "branch2_2 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch2_2\")(branch2_1)\n",
    "branch2_3 = keras.layers.Conv1D(filters=24, kernel_size=8, activation='relu', name =\"branch2_3\")(branch2_2)\n",
    "branch2_4 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch2_4\")(branch2_3)\n",
    "\n",
    "# Pipeline 3\n",
    "branch3_1 = keras.layers.Conv1D(filters=8, kernel_size=8, activation='relu', name =\"branch3_1\")(input_tot)\n",
    "branch3_2 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch3_2\")(branch3_1)\n",
    "branch3_3 = keras.layers.Conv1D(filters=24, kernel_size=10, activation='relu', name =\"branch3_3\")(branch3_2)\n",
    "branch3_4 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch3_4\")(branch3_3)\n",
    "\n",
    "#Merging tre pipeline\n",
    "branch_concatenate = concatenate([branch1_4,branch2_4,branch3_4], axis=1, name=\"concatenated_layer\")\n",
    "\n",
    "#Final Layer\n",
    "dense1 = Dense(256, activation = \"sigmoid\", name = \"dense1\")(branch_concatenate)\n",
    "dense2 = Dense(32, activation = \"sigmoid\", name = \"dense2\")(dense1)\n",
    "output_layer = Dense(4, activation = \"sigmoid\", name = \"output_layer\")(dense2)\n",
    "\n",
    "#Model Definition\n",
    "merged = Model(inputs=[input_tot],outputs=[output_layer], name = \"merged model\")\n",
    "\n",
    "#Model Details\n",
    "merged.summary()\n",
    "keras.utils.plot_model(merged, \"output/architecture.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heartbeats/heartbeats_datasets.csv')\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe012f5",
   "metadata": {},
   "source": [
    "# Addestramento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = (251, 1)\n",
    "model = create_lstm_model(input, 2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b6308",
   "metadata": {},
   "source": [
    "# Addestramento 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f296ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "def swish(x):\n",
    "    return x * K.sigmoid(x)\n",
    "\n",
    "y = swish(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('swish(x)')\n",
    "plt.title('Swish Function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, LSTM, GlobalAveragePooling1D, Dense, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "def create_model(input_shape, n_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(32, kernel_size=3, strides=1, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    x = Conv1D(128, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=2, strides=2, padding='same')(x)\n",
    "\n",
    "    print(x)\n",
    "\n",
    "\n",
    "\n",
    "    x = LSTM(128, return_sequences=True, go_backwards=False, dropout=0.5)(x)\n",
    "    x = LSTM(128, return_sequences=False, go_backwards=True, dropout=0.5)(x)\n",
    "\n",
    "    x = Reshape((-1, 128))(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    outputs = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = (251, 1)\n",
    "model = create_model(input, 2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff6ea8",
   "metadata": {},
   "source": [
    "# Addestramento 5 (Autoencoders)\n",
    "## Vedere anche:\n",
    "## https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef5ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Dimensione dell'input (lunghezza del segnale ECG)\n",
    "input_dim = 300\n",
    "\n",
    "# Definire l'encoder\n",
    "input_data = Input(shape=(input_dim,))\n",
    "encoded = Dense(100, activation='relu')(input_data)\n",
    "encoded = Dense(50, activation='relu')(encoded)\n",
    "# Definire il decoder\n",
    "decoded = Dense(100, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# Creare l'autoencoder\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "# Compilare l'autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Addestrare l'autoencoder sui dati di input\n",
    "# autoencoder.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Utilizzare l'encoder per ottenere la rappresentazione codificata dei dati di input\n",
    "encoder = Model(input_data, encoded)\n",
    "encoded_input = encoder.predict(X_train)\n",
    "encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# print(encoded_input)\n",
    "# print(type(encoded_input))\n",
    "# print(len(encoded_input))\n",
    "# print(len(encoded_input[0]))\n",
    "\n",
    "encoded_dim = 50\n",
    "num_classes = 2\n",
    "\n",
    "# Utilizzare la rappresentazione codificata come input per la classificazione\n",
    "input_data = Input(shape=(encoded_dim,))\n",
    "classification = Dense(100, activation='relu')(input_data)\n",
    "classification = Dense(50, activation='relu')(classification)\n",
    "classification = Dense(num_classes, activation='softmax')(classification)\n",
    "classifier = Model(input_data, classification)\n",
    "\n",
    "# Compilare la rete neurale classificatrice\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestrare la rete neurale classificatrice sulla rappresentazione codificata\n",
    "classifier.fit(encoded_input, y_train, epochs=100, batch_size=64, validation_data=(encoded_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64925b87",
   "metadata": {},
   "source": [
    "### Qui sotto è definito l'autoencoder per vedere i suoi parametri per ogni livello e generali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Dimensione dell'input (lunghezza del segnale ECG)\n",
    "input_dim = 300\n",
    "\n",
    "# Definire l'encoder\n",
    "input_data = Input(shape=(input_dim,))\n",
    "encoded = Dense(100, activation='relu')(input_data)\n",
    "encoded = Dense(50, activation='relu')(encoded)\n",
    "# Definire il decoder\n",
    "decoded = Dense(100, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# Creare l'autoencoder\n",
    "autoencoder = Model(input_data, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b567c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilare l'autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# Addestrare l'autoencoder sui dati di input\n",
    "autoencoder.fit(X_train, y_train, epochs=3, batch_size=32)\n",
    "# DA VERIFICARE SE UTILIZZARE ANCHE IL VALIDATION_DATA\n",
    "# autoencoder.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzare l'encoder per ottenere la rappresentazione codificata dei dati di input\n",
    "encoder = Model(input_data, encoded)\n",
    "encoded_input = encoder.predict(X_train)\n",
    "\n",
    "# Utilizzare la rappresentazione codificata come input per la classificazione\n",
    "input_data = Input(shape=(encoded_dim,))\n",
    "classification = Dense(100, activation='relu')(input_data)\n",
    "classification = Dense(50, activation='relu')(classification)\n",
    "classification = Dense(num_classes, activation='softmax')(classification)\n",
    "classifier = Model(input_data, classification)\n",
    "\n",
    "# Compilare la rete neurale classificatrice\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestrare la rete neurale classificatrice sulla rappresentazione codificata\n",
    "classifier.fit(encoded_input, y_train, epochs=100, batch_size=32, validation_data=(encoded_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca703ff",
   "metadata": {},
   "source": [
    "# DBN\n",
    "\n",
    "#### Deep Belief Networks (DBN) - Le DBN sono composte da più strati di unità nascoste, che sono utilizzati per l'estrazione di caratteristiche del segnale. Possono essere utilizzate per la classificazione delle aritmie e per la predizione della mortalità a breve termine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Definisci l'architettura del DBN\n",
    "visible = Input(shape=(300,))\n",
    "hidden1 = Dense(256, activation='relu')(visible)\n",
    "dropout1 = Dropout(0.2)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.2)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(dropout2)\n",
    "dropout3 = Dropout(0.2)(hidden3)\n",
    "output = Dense(2, activation='softmax')(dropout3)\n",
    "\n",
    "# Crea il modello del DBN\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# Compila il modello\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "# Addestra il modello\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "# history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stop])\n",
    "\n",
    "# Valuta il modello\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "# print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ba9f5",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import delle librerie\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding1D, UpSampling1D, Conv1D, LeakyReLU\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Definizione del generatore\n",
    "def build_generator():\n",
    "\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 16, activation=\"relu\", input_shape=noise_shape))\n",
    "    model.add(Reshape((128, 16)))\n",
    "    model.add(UpSampling1D())\n",
    "    model.add(Conv1D(64, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling1D())\n",
    "    model.add(Conv1D(32, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv1D(1, kernel_size=4, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    signal = model(noise)\n",
    "\n",
    "    return Model(noise, signal)\n",
    "\n",
    "# Definizione del discriminatore\n",
    "def build_discriminator():\n",
    "\n",
    "    signal_shape = (300, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=4, strides=2, input_shape=signal_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding1D(padding=(0, 1)))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(256, kernel_size=4, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    signal = Input(shape=signal_shape)\n",
    "    validity = model(signal)\n",
    "\n",
    "    return Model(signal, validity)\n",
    "\n",
    "# Compilazione della GAN\n",
    "def build_gan(generator, discriminator):\n",
    "\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    noise = Input(shape=(100,))\n",
    "    signal = generator(noise)\n",
    "    validity = discriminator(signal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf87867",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_G = tf.keras.layers.Dense(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_D = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"tanh\", input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(3, activation=\"tanh\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def update_D(X, Z, net_D, net_G, loss, optimizer_D):\n",
    "    \"\"\"Update discriminator.\"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    ones = tf.ones((batch_size,)) # Labels corresponding to real data\n",
    "    zeros = tf.zeros((batch_size,)) # Labels corresponding to fake data\n",
    "    # Do not need to compute gradient for `net_G`, so it is outside GradientTape\n",
    "    fake_X = net_G(Z)\n",
    "    with tf.GradientTape() as tape:\n",
    "        real_Y = net_D(X)\n",
    "        fake_Y = net_D(fake_X)\n",
    "        # We multiply the loss by batch_size to match PyTorch's BCEWithLogitsLoss\n",
    "        loss_D = (loss(ones, tf.squeeze(real_Y)) + loss(\n",
    "            zeros, tf.squeeze(fake_Y))) * batch_size / 2\n",
    "    grads_D = tape.gradient(loss_D, net_D.trainable_variables)\n",
    "    optimizer_D.apply_gradients(zip(grads_D, net_D.trainable_variables))\n",
    "    return loss_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def update_G(Z, net_D, net_G, loss, optimizer_G):\n",
    "    \"\"\"Update generator.\"\"\"\n",
    "    batch_size = Z.shape[0]\n",
    "    ones = tf.ones((batch_size,))\n",
    "    with tf.GradientTape() as tape:\n",
    "        # We could reuse `fake_X` from `update_D` to save computation\n",
    "        fake_X = net_G(Z)\n",
    "        # Recomputing `fake_Y` is needed since `net_D` is changed\n",
    "        fake_Y = net_D(fake_X)\n",
    "        # We multiply the loss by batch_size to match PyTorch's BCEWithLogits loss\n",
    "        loss_G = loss(ones, tf.squeeze(fake_Y)) * batch_size\n",
    "    grads_G = tape.gradient(loss_G, net_G.trainable_variables)\n",
    "    optimizer_G.apply_gradients(zip(grads_G, net_G.trainable_variables))\n",
    "    return loss_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09020403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "    for w in net_D.trainable_variables:\n",
    "        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))\n",
    "    for w in net_G.trainable_variables:\n",
    "        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))\n",
    "    optimizer_D = tf.keras.optimizers.Adam(learning_rate=lr_D)\n",
    "    optimizer_G = tf.keras.optimizers.Adam(learning_rate=lr_G)\n",
    "    animator = d2l.Animator(\n",
    "        xlabel=\"epoch\", ylabel=\"loss\", xlim=[1, num_epochs], nrows=2,\n",
    "        figsize=(5, 5), legend=[\"discriminator\", \"generator\"])\n",
    "    animator.fig.subplots_adjust(hspace=0.3)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train one epoch\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "        for (X,) in data_iter:\n",
    "            batch_size = X.shape[0]\n",
    "            Z = tf.random.normal(\n",
    "                mean=0, stddev=1, shape=(batch_size, latent_dim))\n",
    "            metric.add(update_D(X, Z, net_D, net_G, loss, optimizer_D),\n",
    "                       update_G(Z, net_D, net_G, loss, optimizer_G),\n",
    "                       batch_size)\n",
    "        # Visualize generated examples\n",
    "        Z = tf.random.normal(mean=0, stddev=1, shape=(100, latent_dim))\n",
    "        fake_X = net_G(Z)\n",
    "        animator.axes[1].cla()\n",
    "        animator.axes[1].scatter(data[:, 0], data[:, 1])\n",
    "        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n",
    "        animator.axes[1].legend([\"real\", \"generated\"])\n",
    "\n",
    "        # Show the losses\n",
    "        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n",
    "        animator.add(epoch + 1, (loss_D, loss_G))\n",
    "\n",
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96165028",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_D, lr_G, latent_dim, num_epochs = 0.05, 0.005, 2, 20\n",
    "train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\n",
    "      latent_dim, data[:100].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3dda93",
   "metadata": {},
   "source": [
    "# Rete con utilizzo di sincnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a08c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroid import Lambda\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def sinc(band, x):\n",
    "    return K.sinc(band * (x - (K.pi / 2) / band))\n",
    "\n",
    "def sinc_layer(band):\n",
    "    def func(x):\n",
    "        return sinc(band, x)\n",
    "    return Lambda(func)\n",
    "\n",
    "def build_sinc_net(num_filters, kernel_size, pool_size, num_classes):\n",
    "    # Numero di classi da classificare\n",
    "    num_classes = 5\n",
    "\n",
    "    # Dati di input e target\n",
    "    x_train = np.random.rand(1000, 300, 1)\n",
    "    y_train = to_categorical(np.random.randint(0, num_classes, 1000), num_classes)\n",
    "    x_test = np.random.rand(100, 300, 1)\n",
    "    y_test = to_categorical(np.random.randint(0, num_classes, 100), num_classes)\n",
    "\n",
    "    # Definire l'input\n",
    "    input_data = Input(shape=(300, 1))\n",
    "\n",
    "    # Definire le convolutional layer\n",
    "    conv_1 = Conv1D(num_filters, kernel_size, activation=sinc_layer(kernel_size), padding=\"same\")(input_data)\n",
    "    pool_1 = MaxPooling1D(pool_size)(conv_1)\n",
    "    conv_2 = Conv1D(num_filters, kernel_size, activation=sinc_layer(kernel_size), padding=\"same\")(pool_1)\n",
    "    pool_2 = MaxPooling1D(pool_size)(conv_2)\n",
    "\n",
    "    # Definire la fully connected layer\n",
    "    flat = Flatten()(pool_2)\n",
    "    dense = Dense(100, activation='relu')(flat)\n",
    "    output = Dense(num_classes, activation='softmax')(dense)\n",
    "\n",
    "    # Creare il modello SincNet\n",
    "    sinc_net = Model(input_data, output)\n",
    "\n",
    "    # Compilare il modello SincNet\n",
    "    sinc_net.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Addestrare il modello SincNet sui dati di input\n",
    "    sinc_net.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "    return sinc_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinc_net = build_sinc_net(32, 3, 2, 2)\n",
    "sinc_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30492308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def sinc(X, axis):\n",
    "    return np.sin(np.pi * X) / (np.pi * X)\n",
    "\n",
    "# def sinc_conv(X, num_filters, kernel_size, dilation_rate, name):\n",
    "#     sinc_init = np.zeros((kernel_size, 1, num_filters))\n",
    "#     for i in range(num_filters):\n",
    "#         sinc_init[:, 0, i] = sinc(np.linspace(-kernel_size//2 + 1, kernel_size//2 + 1, kernel_size), dilation_rate * (i + 1))\n",
    "#     sinc_init = sinc_init / np.linalg.norm(sinc_init, axis=(0, 1))\n",
    "#     return Conv1D(num_filters, kernel_size, dilation_rate=dilation_rate, activation='relu', kernel_initializer=keras.initializers.Constant(value=sinc_init), name=name)(X)\n",
    "\n",
    "def sinc_conv(X, num_filters, kernel_size, dilation_rate, name):\n",
    "    sinc_init = np.zeros((kernel_size, 1, num_filters))\n",
    "    for i in range(num_filters):\n",
    "        sinc_init[:, 0, i] = sinc(np.linspace(-kernel_size//2 + 1, kernel_size//2 + 1, kernel_size), dilation_rate * (i + 1))\n",
    "    sinc_init = sinc_init / np.linalg.norm(sinc_init, axis=(0, 1))\n",
    "    sinc_init = sinc_init.reshape((kernel_size, num_filters))\n",
    "    return Conv1D(num_filters, kernel_size, dilation_rate=dilation_rate, activation='relu', kernel_initializer=keras.initializers.Constant(value=sinc_init), name=name)(X)\n",
    "\n",
    "\n",
    "def build_sincnet():\n",
    "    inputs = Input(shape=(300, 1))\n",
    "    X = sinc_conv(inputs, 32, 7, 1, 'sinc_conv1')\n",
    "    X = MaxPooling1D(pool_size=3, strides=3, padding='same')(X)\n",
    "    X = sinc_conv(X, 64, 7, 2, 'sinc_conv2')\n",
    "    X = MaxPooling1D(pool_size=3, strides=3, padding='same')(X)\n",
    "    X = sinc_conv(X, 128, 7, 4, 'sinc_conv3')\n",
    "    X = MaxPooling1D(pool_size=3, strides=3, padding='same')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    outputs = Dense(2, activation='sigmoid')(X)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_sincnet()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93786e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12621a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
