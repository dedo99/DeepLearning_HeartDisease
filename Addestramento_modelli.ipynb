{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edc8066",
   "metadata": {},
   "source": [
    "# Addestramento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7922fa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: d2l==1.0.0a1.post0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (1.0.0a1.post0)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from d2l==1.0.0a1.post0) (1.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\anaconda3\\lib\\site-packages (from d2l==1.0.0a1.post0) (3.4.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\andre\\anaconda3\\lib\\site-packages (from d2l==1.0.0a1.post0) (1.3.4)\n",
      "Requirement already satisfied: gym in c:\\users\\andre\\anaconda3\\lib\\site-packages (from d2l==1.0.0a1.post0) (0.26.2)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda3\\lib\\site-packages (from d2l==1.0.0a1.post0) (2.26.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from d2l==1.0.0a1.post0) (0.1.6)\n",
      "Requirement already satisfied: jupyter in c:\\users\\andre\\anaconda3\\lib\\site-packages (from d2l==1.0.0a1.post0) (1.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym->d2l==1.0.0a1.post0) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym->d2l==1.0.0a1.post0) (4.8.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gym->d2l==1.0.0a1.post0) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0a1.post0) (3.6.0)\n",
      "Requirement already satisfied: notebook in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jupyter->d2l==1.0.0a1.post0) (6.4.5)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jupyter->d2l==1.0.0a1.post0) (6.1.0)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jupyter->d2l==1.0.0a1.post0) (6.4.0)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jupyter->d2l==1.0.0a1.post0) (5.1.1)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from jupyter->d2l==1.0.0a1.post0) (6.17.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jupyter->d2l==1.0.0a1.post0) (7.6.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (5.9.4)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (6.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (7.4.8)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (8.7.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (1.6.4)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (24.0.1)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (5.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (21.3)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (1.5.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.18.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.4.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (3.0.36)\n",
      "Requirement already satisfied: stack-data in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.6.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (2.13.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->d2l==1.0.0a1.post0) (5.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->d2l==1.0.0a1.post0) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->jupyter->d2l==1.0.0a1.post0) (2.6.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->jupyter->d2l==1.0.0a1.post0) (305)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter->d2l==1.0.0a1.post0) (1.16.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (3.5.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (0.2.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (5.1.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (1.0.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==1.0.0a1.post0) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==1.0.0a1.post0) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==1.0.0a1.post0) (58.0.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->d2l==1.0.0a1.post0) (21.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (2.11.3)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\andre\\anaconda3\\lib\\site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (20.1.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\andre\\anaconda3\\lib\\site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (0.11.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (0.9.4)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter->d2l==1.0.0a1.post0) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook->jupyter->d2l==1.0.0a1.post0) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\andre\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter->d2l==1.0.0a1.post0) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jinja2->notebook->jupyter->d2l==1.0.0a1.post0) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->d2l==1.0.0a1.post0) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from matplotlib->d2l==1.0.0a1.post0) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from matplotlib->d2l==1.0.0a1.post0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from matplotlib->d2l==1.0.0a1.post0) (1.3.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.5.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (4.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (1.4.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.1.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.7.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\andre\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter->d2l==1.0.0a1.post0) (1.10)\n",
      "Requirement already satisfied: webencodings in c:\\users\\andre\\anaconda3\\lib\\site-packages (from bleach->nbconvert->jupyter->d2l==1.0.0a1.post0) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pandas->d2l==1.0.0a1.post0) (2021.3)\n",
      "Requirement already satisfied: qtpy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from qtconsole->jupyter->d2l==1.0.0a1.post0) (1.10.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->d2l==1.0.0a1.post0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->d2l==1.0.0a1.post0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->d2l==1.0.0a1.post0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->d2l==1.0.0a1.post0) (3.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.legacy_tf_layers.convolutional import Conv1D\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "!pip install d2l==1.0.0a1.post0\n",
    "from d2l import tensorflow as d2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f072ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 296, 64)           384       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 292, 64)           20544     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 146, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 144, 128)          24704     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 142, 128)          49280     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 71, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9088)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2326784   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,455,237\n",
      "Trainable params: 2,455,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"valid\", input_shape=[300, 1]),\n",
    "keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"valid\"),\n",
    "keras.layers.MaxPooling1D(pool_size=2, strides=2),\n",
    "keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"valid\"),\n",
    "keras.layers.Conv1D(filters=128, kernel_size=3, strides=1, padding=\"valid\"),\n",
    "keras.layers.MaxPooling1D(pool_size=2, strides=2),\n",
    "keras.layers.Flatten(),\n",
    "keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c9aee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 2\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(x\u001B[38;5;241m=\u001B[39m\u001B[43mx_train\u001B[49m, y\u001B[38;5;241m=\u001B[39my_train, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m      4\u001B[0m test_error_rate \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(x_test, y_test, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=x_train, y=y_train, epochs=3)\n",
    "\n",
    "test_error_rate = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfbef5",
   "metadata": {},
   "source": [
    "# Addestramento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"merged model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input_tot (InputLayer)         [(None, 251, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " branch1_1 (Conv1D)             (None, 248, 8)       40          ['Input_tot[0][0]']              \n",
      "                                                                                                  \n",
      " branch2_1 (Conv1D)             (None, 246, 8)       56          ['Input_tot[0][0]']              \n",
      "                                                                                                  \n",
      " branch3_1 (Conv1D)             (None, 244, 8)       72          ['Input_tot[0][0]']              \n",
      "                                                                                                  \n",
      " branch1_2 (MaxPooling1D)       (None, 124, 8)       0           ['branch1_1[0][0]']              \n",
      "                                                                                                  \n",
      " branch2_2 (MaxPooling1D)       (None, 123, 8)       0           ['branch2_1[0][0]']              \n",
      "                                                                                                  \n",
      " branch3_2 (MaxPooling1D)       (None, 122, 8)       0           ['branch3_1[0][0]']              \n",
      "                                                                                                  \n",
      " branch1_3 (Conv1D)             (None, 119, 24)      1176        ['branch1_2[0][0]']              \n",
      "                                                                                                  \n",
      " branch2_3 (Conv1D)             (None, 116, 24)      1560        ['branch2_2[0][0]']              \n",
      "                                                                                                  \n",
      " branch3_3 (Conv1D)             (None, 113, 24)      1944        ['branch3_2[0][0]']              \n",
      "                                                                                                  \n",
      " branch1_4 (MaxPooling1D)       (None, 59, 24)       0           ['branch1_3[0][0]']              \n",
      "                                                                                                  \n",
      " branch2_4 (MaxPooling1D)       (None, 58, 24)       0           ['branch2_3[0][0]']              \n",
      "                                                                                                  \n",
      " branch3_4 (MaxPooling1D)       (None, 56, 24)       0           ['branch3_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenated_layer (Concatenat  (None, 173, 24)     0           ['branch1_4[0][0]',              \n",
      " e)                                                               'branch2_4[0][0]',              \n",
      "                                                                  'branch3_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 173, 256)     6400        ['concatenated_layer[0][0]']     \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 173, 32)      8224        ['dense1[0][0]']                 \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 173, 4)       132         ['dense2[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,604\n",
      "Trainable params: 19,604\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "input_tot = Input(shape=(251, 1), name =\"Input_tot\")\n",
    "\n",
    "# Pipeline 1\n",
    "branch1_1 = keras.layers.Conv1D(filters=8, kernel_size=4, activation='relu', name =\"branch1_1\")(input_tot)\n",
    "branch1_2 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch1_2\")(branch1_1)\n",
    "branch1_3 = keras.layers.Conv1D(filters=24, kernel_size=6, activation='relu', name =\"branch1_3\")(branch1_2)\n",
    "branch1_4 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch1_4\")(branch1_3)\n",
    "\n",
    "# Pipeline 2\n",
    "branch2_1 = keras.layers.Conv1D(filters=8, kernel_size=6, activation='relu', name =\"branch2_1\")(input_tot)\n",
    "branch2_2 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch2_2\")(branch2_1)\n",
    "branch2_3 = keras.layers.Conv1D(filters=24, kernel_size=8, activation='relu', name =\"branch2_3\")(branch2_2)\n",
    "branch2_4 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch2_4\")(branch2_3)\n",
    "\n",
    "# Pipeline 3\n",
    "branch3_1 = keras.layers.Conv1D(filters=8, kernel_size=8, activation='relu', name =\"branch3_1\")(input_tot)\n",
    "branch3_2 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch3_2\")(branch3_1)\n",
    "branch3_3 = keras.layers.Conv1D(filters=24, kernel_size=10, activation='relu', name =\"branch3_3\")(branch3_2)\n",
    "branch3_4 = keras.layers.MaxPooling1D(pool_size=2, strides=2, name =\"branch3_4\")(branch3_3)\n",
    "\n",
    "#Merging tre pipeline\n",
    "branch_concatenate = concatenate([branch1_4,branch2_4,branch3_4], axis=1, name=\"concatenated_layer\")\n",
    "\n",
    "#Final Layer\n",
    "dense1 = Dense(256, activation = \"sigmoid\", name = \"dense1\")(branch_concatenate)\n",
    "dense2 = Dense(32, activation = \"sigmoid\", name = \"dense2\")(dense1)\n",
    "output_layer = Dense(4, activation = \"sigmoid\", name = \"output_layer\")(dense2)\n",
    "\n",
    "#Model Definition\n",
    "merged = Model(inputs=[input_tot],outputs=[output_layer], name = \"merged model\")\n",
    "\n",
    "#Model Details\n",
    "merged.summary()\n",
    "keras.utils.plot_model(merged, \"output/architecture.png\", show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              signal annotypes\n0  [-0.265 -0.27  -0.265 -0.26  -0.275 -0.28  -0....         N",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>signal</th>\n      <th>annotypes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.265 -0.27  -0.265 -0.26  -0.275 -0.28  -0....</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('heartbeats/heartbeats_datasets.csv')\n",
    "\n",
    "df.head(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Addestramento 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 251, 128)          66560     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 251, 128)          0         \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,402\n",
      "Trainable params: 198,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = (251, 1)\n",
    "model = create_lstm_model(input, 2)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Addestramento 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "def swish(x):\n",
    "    return x * K.sigmoid(x)\n",
    "\n",
    "y = swish(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('swish(x)')\n",
    "plt.title('Swish Function')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, LSTM, GlobalAveragePooling1D, Dense, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "def create_model(input_shape, n_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(32, kernel_size=3, strides=1, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    x = Conv1D(128, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=2, strides=2, padding='same')(x)\n",
    "\n",
    "    print(x)\n",
    "\n",
    "\n",
    "\n",
    "    x = LSTM(128, return_sequences=True, go_backwards=False, dropout=0.5)(x)\n",
    "    x = LSTM(128, return_sequences=False, go_backwards=True, dropout=0.5)(x)\n",
    "\n",
    "    x = Reshape((-1, 128))(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    outputs = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 126, 128), dtype=tf.float32, name=None), name='max_pooling1d_5/Squeeze:0', description=\"created by layer 'max_pooling1d_5'\")\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 251, 1)]          0         \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 251, 32)           128       \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 251, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 251, 32)           0         \n",
      "                                                                 \n",
      " conv1d_20 (Conv1D)          (None, 251, 64)           6208      \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 251, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 251, 64)           0         \n",
      "                                                                 \n",
      " conv1d_21 (Conv1D)          (None, 251, 128)          24704     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 251, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 251, 128)          0         \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 126, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_21 (LSTM)              (None, 126, 128)          131584    \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 128)               131584    \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 1, 128)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d_7   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 295,362\n",
      "Trainable params: 294,914\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = (251, 1)\n",
    "model = create_model(input, 2)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Addestramento 5 (Autoencoders)\n",
    "## Vedere anche:\n",
    "## https://blog.keras.io/building-autoencoders-in-keras.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Dimensione dell'input (lunghezza del segnale ECG)\n",
    "input_dim = 300\n",
    "\n",
    "# Definire l'encoder\n",
    "input_data = Input(shape=(input_dim,))\n",
    "encoded = Dense(100, activation='relu')(input_data)\n",
    "encoded = Dense(50, activation='relu')(encoded)\n",
    "# Definire il decoder\n",
    "decoded = Dense(100, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# Creare l'autoencoder\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "# Compilare l'autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Addestrare l'autoencoder sui dati di input\n",
    "# autoencoder.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Utilizzare l'encoder per ottenere la rappresentazione codificata dei dati di input\n",
    "encoder = Model(input_data, encoded)\n",
    "encoded_input = encoder.predict(X_train)\n",
    "encoded_test = encoder.predict(X_test)\n",
    "\n",
    "# print(encoded_input)\n",
    "# print(type(encoded_input))\n",
    "# print(len(encoded_input))\n",
    "# print(len(encoded_input[0]))\n",
    "\n",
    "encoded_dim = 50\n",
    "num_classes = 2\n",
    "\n",
    "# Utilizzare la rappresentazione codificata come input per la classificazione\n",
    "input_data = Input(shape=(encoded_dim,))\n",
    "classification = Dense(100, activation='relu')(input_data)\n",
    "classification = Dense(50, activation='relu')(classification)\n",
    "classification = Dense(num_classes, activation='softmax')(classification)\n",
    "classifier = Model(input_data, classification)\n",
    "\n",
    "# Compilare la rete neurale classificatrice\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestrare la rete neurale classificatrice sulla rappresentazione codificata\n",
    "classifier.fit(encoded_input, y_train, epochs=3, batch_size=64, validation_data=(encoded_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Qui sotto  definito l'autoencoder per vedere i suoi parametri per ogni livello e generali"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               5100      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 300)               30300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,550\n",
      "Trainable params: 70,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Dimensione dell'input (lunghezza del segnale ECG)\n",
    "input_dim = 300\n",
    "\n",
    "# Definire l'encoder\n",
    "input_data = Input(shape=(input_dim,))\n",
    "encoded = Dense(100, activation='relu')(input_data)\n",
    "encoded = Dense(50, activation='relu')(encoded)\n",
    "# Definire il decoder\n",
    "decoded = Dense(100, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# Creare l'autoencoder\n",
    "autoencoder = Model(input_data, decoded)\n",
    "autoencoder.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compilare l'autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# Addestrare l'autoencoder sui dati di input\n",
    "autoencoder.fit(X_train, y_train, epochs=3, batch_size=32)\n",
    "# DA VERIFICARE SE UTILIZZARE ANCHE IL VALIDATION_DATA\n",
    "# autoencoder.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(x_test, x_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m autoencoder\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Addestrare l'autoencoder sui dati di input\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m autoencoder\u001B[38;5;241m.\u001B[39mfit(\u001B[43mX_train\u001B[49m, y_train, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# DA VERIFICARE SE UTILIZZARE ANCHE IL VALIDATION_DATA\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# autoencoder.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(x_test, x_test))\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Utilizzare l'encoder per ottenere la rappresentazione codificata dei dati di input\u001B[39;00m\n\u001B[0;32m     28\u001B[0m encoder \u001B[38;5;241m=\u001B[39m Model(input_data, encoded)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Utilizzare l'encoder per ottenere la rappresentazione codificata dei dati di input\n",
    "encoder = Model(input_data, encoded)\n",
    "encoded_input = encoder.predict(X_train)\n",
    "\n",
    "# Utilizzare la rappresentazione codificata come input per la classificazione\n",
    "input_data = Input(shape=(encoded_dim,))\n",
    "classification = Dense(100, activation='relu')(input_data)\n",
    "classification = Dense(50, activation='relu')(classification)\n",
    "classification = Dense(num_classes, activation='softmax')(classification)\n",
    "classifier = Model(input_data, classification)\n",
    "\n",
    "# Compilare la rete neurale classificatrice\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestrare la rete neurale classificatrice sulla rappresentazione codificata\n",
    "classifier.fit(encoded_input, y_train, epochs=50, batch_size=32, validation_data=(encoded_test, y_test))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DBN\n",
    "\n",
    "#### Deep Belief Networks (DBN) - Le DBN sono composte da pi strati di unit nascoste, che sono utilizzati per l'estrazione di caratteristiche del segnale. Possono essere utilizzate per la classificazione delle aritmie e per la predizione della mortalit a breve termine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               77056     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,338\n",
      "Trainable params: 118,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Definisci l'architettura del DBN\n",
    "visible = Input(shape=(300,))\n",
    "hidden1 = Dense(256, activation='relu')(visible)\n",
    "dropout1 = Dropout(0.2)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.2)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(dropout2)\n",
    "dropout3 = Dropout(0.2)(hidden3)\n",
    "output = Dense(2, activation='softmax')(dropout3)\n",
    "\n",
    "# Crea il modello del DBN\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# Compila il modello\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "# Addestra il modello\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "# history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stop])\n",
    "\n",
    "# Valuta il modello\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "# print('Test accuracy:', test_acc)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Import delle librerie\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding1D, UpSampling1D, Conv1D, LeakyReLU\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Definizione del generatore\n",
    "def build_generator():\n",
    "\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 16, activation=\"relu\", input_shape=noise_shape))\n",
    "    model.add(Reshape((128, 16)))\n",
    "    model.add(UpSampling1D())\n",
    "    model.add(Conv1D(64, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling1D())\n",
    "    model.add(Conv1D(32, kernel_size=4, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv1D(1, kernel_size=4, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    signal = model(noise)\n",
    "\n",
    "    return Model(noise, signal)\n",
    "\n",
    "# Definizione del discriminatore\n",
    "def build_discriminator():\n",
    "\n",
    "    signal_shape = (128, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=4, strides=2, input_shape=signal_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding1D(padding=(0, 1)))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(256, kernel_size=4, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    signal = Input(shape=signal_shape)\n",
    "    validity = model(signal)\n",
    "\n",
    "    return Model(signal, validity)\n",
    "\n",
    "# Compilazione della GAN\n",
    "def build_gan(generator, discriminator):\n",
    "\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    noise = Input(shape=(100,))\n",
    "    signal = generator(noise)\n",
    "    validity = discriminator(signal)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "net_G = tf.keras.layers.Dense(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "net_D = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"tanh\", input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(3, activation=\"tanh\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#@save\n",
    "def update_D(X, Z, net_D, net_G, loss, optimizer_D):\n",
    "    \"\"\"Update discriminator.\"\"\"\n",
    "    batch_size = X.shape[0]\n",
    "    ones = tf.ones((batch_size,)) # Labels corresponding to real data\n",
    "    zeros = tf.zeros((batch_size,)) # Labels corresponding to fake data\n",
    "    # Do not need to compute gradient for `net_G`, so it is outside GradientTape\n",
    "    fake_X = net_G(Z)\n",
    "    with tf.GradientTape() as tape:\n",
    "        real_Y = net_D(X)\n",
    "        fake_Y = net_D(fake_X)\n",
    "        # We multiply the loss by batch_size to match PyTorch's BCEWithLogitsLoss\n",
    "        loss_D = (loss(ones, tf.squeeze(real_Y)) + loss(\n",
    "            zeros, tf.squeeze(fake_Y))) * batch_size / 2\n",
    "    grads_D = tape.gradient(loss_D, net_D.trainable_variables)\n",
    "    optimizer_D.apply_gradients(zip(grads_D, net_D.trainable_variables))\n",
    "    return loss_D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#@save\n",
    "def update_G(Z, net_D, net_G, loss, optimizer_G):\n",
    "    \"\"\"Update generator.\"\"\"\n",
    "    batch_size = Z.shape[0]\n",
    "    ones = tf.ones((batch_size,))\n",
    "    with tf.GradientTape() as tape:\n",
    "        # We could reuse `fake_X` from `update_D` to save computation\n",
    "        fake_X = net_G(Z)\n",
    "        # Recomputing `fake_Y` is needed since `net_D` is changed\n",
    "        fake_Y = net_D(fake_X)\n",
    "        # We multiply the loss by batch_size to match PyTorch's BCEWithLogits loss\n",
    "        loss_G = loss(ones, tf.squeeze(fake_Y)) * batch_size\n",
    "    grads_G = tape.gradient(loss_G, net_G.trainable_variables)\n",
    "    optimizer_G.apply_gradients(zip(grads_G, net_G.trainable_variables))\n",
    "    return loss_G"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "    for w in net_D.trainable_variables:\n",
    "        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))\n",
    "    for w in net_G.trainable_variables:\n",
    "        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))\n",
    "    optimizer_D = tf.keras.optimizers.Adam(learning_rate=lr_D)\n",
    "    optimizer_G = tf.keras.optimizers.Adam(learning_rate=lr_G)\n",
    "    animator = d2l.Animator(\n",
    "        xlabel=\"epoch\", ylabel=\"loss\", xlim=[1, num_epochs], nrows=2,\n",
    "        figsize=(5, 5), legend=[\"discriminator\", \"generator\"])\n",
    "    animator.fig.subplots_adjust(hspace=0.3)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train one epoch\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "        for (X,) in data_iter:\n",
    "            batch_size = X.shape[0]\n",
    "            Z = tf.random.normal(\n",
    "                mean=0, stddev=1, shape=(batch_size, latent_dim))\n",
    "            metric.add(update_D(X, Z, net_D, net_G, loss, optimizer_D),\n",
    "                       update_G(Z, net_D, net_G, loss, optimizer_G),\n",
    "                       batch_size)\n",
    "        # Visualize generated examples\n",
    "        Z = tf.random.normal(mean=0, stddev=1, shape=(100, latent_dim))\n",
    "        fake_X = net_G(Z)\n",
    "        animator.axes[1].cla()\n",
    "        animator.axes[1].scatter(data[:, 0], data[:, 1])\n",
    "        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n",
    "        animator.axes[1].legend([\"real\", \"generated\"])\n",
    "\n",
    "        # Show the losses\n",
    "        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n",
    "        animator.add(epoch + 1, (loss_D, loss_G))\n",
    "\n",
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr_D, lr_G, latent_dim, num_epochs = 0.05, 0.005, 2, 20\n",
    "train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\n",
    "      latent_dim, data[:100].numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rete con utilizzo di sincnet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from astroid import Lambda\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def sinc(band, x):\n",
    "    return K.sinc(band * (x - (K.pi / 2) / band))\n",
    "\n",
    "def sinc_layer(band):\n",
    "    def func(x):\n",
    "        return sinc(band, x)\n",
    "    return Lambda(func)\n",
    "\n",
    "def build_sinc_net(num_filters, kernel_size, pool_size, num_classes):\n",
    "    # Numero di classi da classificare\n",
    "    num_classes = 5\n",
    "\n",
    "    # Dati di input e target\n",
    "    x_train = np.random.rand(1000, 300, 1)\n",
    "    y_train = to_categorical(np.random.randint(0, num_classes, 1000), num_classes)\n",
    "    x_test = np.random.rand(100, 300, 1)\n",
    "    y_test = to_categorical(np.random.randint(0, num_classes, 100), num_classes)\n",
    "\n",
    "    # Definire l'input\n",
    "    input_data = Input(shape=(300, 1))\n",
    "\n",
    "    # Definire le convolutional layer\n",
    "    conv_1 = Conv1D(num_filters, kernel_size, activation=sinc_layer(kernel_size), padding=\"same\")(input_data)\n",
    "    pool_1 = MaxPooling1D(pool_size)(conv_1)\n",
    "    conv_2 = Conv1D(num_filters, kernel_size, activation=sinc_layer(kernel_size), padding=\"same\")(pool_1)\n",
    "    pool_2 = MaxPooling1D(pool_size)(conv_2)\n",
    "\n",
    "    # Definire la fully connected layer\n",
    "    flat = Flatten()(pool_2)\n",
    "    dense = Dense(100, activation='relu')(flat)\n",
    "    output = Dense(num_classes, activation='softmax')(dense)\n",
    "\n",
    "    # Creare il modello SincNet\n",
    "    sinc_net = Model(input_data, output)\n",
    "\n",
    "    # Compilare il modello SincNet\n",
    "    sinc_net.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Addestrare il modello SincNet sui dati di input\n",
    "    sinc_net.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "    return sinc_net\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"lambda_4\" \"                 f\"(type Lambda).\n\nmodule 'keras.backend' has no attribute 'sinc'\n\nCall arguments received by layer \"lambda_4\" \"                 f\"(type Lambda):\n   inputs=tf.Tensor(shape=(None, 300, 32), dtype=float32)\n   mask=None\n   training=False",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m sinc_net \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_sinc_net\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m sinc_net\u001B[38;5;241m.\u001B[39msummary()\n",
      "Cell \u001B[1;32mIn[25], line 31\u001B[0m, in \u001B[0;36mbuild_sinc_net\u001B[1;34m(num_filters, kernel_size, pool_size, num_classes)\u001B[0m\n\u001B[0;32m     28\u001B[0m input_data \u001B[38;5;241m=\u001B[39m Input(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m300\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Definire le convolutional layer\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m conv_1 \u001B[38;5;241m=\u001B[39m \u001B[43mConv1D\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_filters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msinc_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msame\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m pool_1 \u001B[38;5;241m=\u001B[39m MaxPooling1D(pool_size)(conv_1)\n\u001B[0;32m     33\u001B[0m conv_2 \u001B[38;5;241m=\u001B[39m Conv1D(num_filters, kernel_size, activation\u001B[38;5;241m=\u001B[39msinc_layer(kernel_size), padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m\"\u001B[39m)(pool_1)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[1;32mIn[25], line 14\u001B[0m, in \u001B[0;36msinc_layer.<locals>.func\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(x):\n\u001B[1;32m---> 14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msinc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mband\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[25], line 10\u001B[0m, in \u001B[0;36msinc\u001B[1;34m(band, x)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msinc\u001B[39m(band, x):\n\u001B[1;32m---> 10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mK\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msinc\u001B[49m(band \u001B[38;5;241m*\u001B[39m (x \u001B[38;5;241m-\u001B[39m (K\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m/\u001B[39m band))\n",
      "\u001B[1;31mAttributeError\u001B[0m: Exception encountered when calling layer \"lambda_4\" \"                 f\"(type Lambda).\n\nmodule 'keras.backend' has no attribute 'sinc'\n\nCall arguments received by layer \"lambda_4\" \"                 f\"(type Lambda):\n   inputs=tf.Tensor(shape=(None, 300, 32), dtype=float32)\n   mask=None\n   training=False"
     ]
    }
   ],
   "source": [
    "sinc_net = build_sinc_net(32, 3, 2, 2)\n",
    "sinc_net.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Eager execution of tf.constant with unsupported shape. Tensor [[ 4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17]\n [-1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01]\n [ 5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01]\n [ 8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01]\n [-2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01]\n [ 7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02]\n [-4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17]] (converted from [[ 4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17]\n [-1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01]\n [ 5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01]\n [ 8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01]\n [-2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01]\n [ 7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02]\n [-4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17]]) has 448 elements, but got `shape` (7, 32, 64) with 14336 elements).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 42\u001B[0m\n\u001B[0;32m     39\u001B[0m     model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39mAdam(), metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[1;32m---> 42\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_sincnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# model.summary()\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[14], line 30\u001B[0m, in \u001B[0;36mbuild_sincnet\u001B[1;34m()\u001B[0m\n\u001B[0;32m     28\u001B[0m X \u001B[38;5;241m=\u001B[39m sinc_conv(inputs, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msinc_conv1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     29\u001B[0m X \u001B[38;5;241m=\u001B[39m MaxPooling1D(pool_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, strides\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m'\u001B[39m)(X)\n\u001B[1;32m---> 30\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43msinc_conv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msinc_conv2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m X \u001B[38;5;241m=\u001B[39m MaxPooling1D(pool_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, strides\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m'\u001B[39m)(X)\n\u001B[0;32m     32\u001B[0m X \u001B[38;5;241m=\u001B[39m sinc_conv(X, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msinc_conv3\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[14], line 23\u001B[0m, in \u001B[0;36msinc_conv\u001B[1;34m(X, num_filters, kernel_size, dilation_rate, name)\u001B[0m\n\u001B[0;32m     21\u001B[0m sinc_init \u001B[38;5;241m=\u001B[39m sinc_init \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(sinc_init, axis\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     22\u001B[0m sinc_init \u001B[38;5;241m=\u001B[39m sinc_init\u001B[38;5;241m.\u001B[39mreshape((kernel_size, num_filters))\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mConv1D\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_filters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdilation_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdilation_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrelu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_initializer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitializers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mConstant\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msinc_init\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:265\u001B[0m, in \u001B[0;36mConstant.__call__\u001B[1;34m(self, shape, dtype, **kwargs)\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m layout:\n\u001B[0;32m    262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m utils\u001B[38;5;241m.\u001B[39mcall_with_layout(\n\u001B[0;32m    263\u001B[0m         tf\u001B[38;5;241m.\u001B[39mconstant, layout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue, shape\u001B[38;5;241m=\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mdtype\n\u001B[0;32m    264\u001B[0m     )\n\u001B[1;32m--> 265\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconstant\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_get_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: Eager execution of tf.constant with unsupported shape. Tensor [[ 4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17\n   4.97354751e-17  4.97354751e-17  4.97354751e-17  4.97354751e-17]\n [-1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01\n  -1.10760264e-01 -1.10760264e-01 -1.10760264e-01 -1.10760264e-01]\n [ 5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01\n   5.27566612e-01  5.27566612e-01  5.27566612e-01  5.27566612e-01]\n [ 8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01\n   8.12241971e-01  8.12241971e-01  8.12241971e-01  8.12241971e-01]\n [-2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01\n  -2.11026654e-01 -2.11026654e-01 -2.11026654e-01 -2.11026654e-01]\n [ 7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02\n   7.16684088e-02  7.16684088e-02  7.16684088e-02  7.16684088e-02]\n [-4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17\n  -4.97354751e-17 -4.97354751e-17 -4.97354751e-17 -4.97354751e-17]] (converted from [[ 4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17\n   4.97354760e-17  4.97354760e-17  4.97354760e-17  4.97354760e-17]\n [-1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01\n  -1.10760268e-01 -1.10760268e-01 -1.10760268e-01 -1.10760268e-01]\n [ 5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01\n   5.27566630e-01  5.27566630e-01  5.27566630e-01  5.27566630e-01]\n [ 8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01\n   8.12241962e-01  8.12241962e-01  8.12241962e-01  8.12241962e-01]\n [-2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01\n  -2.11026652e-01 -2.11026652e-01 -2.11026652e-01 -2.11026652e-01]\n [ 7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02\n   7.16684084e-02  7.16684084e-02  7.16684084e-02  7.16684084e-02]\n [-4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17\n  -4.97354760e-17 -4.97354760e-17 -4.97354760e-17 -4.97354760e-17]]) has 448 elements, but got `shape` (7, 32, 64) with 14336 elements)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def sinc(X, axis):\n",
    "    return np.sin(np.pi * X) / (np.pi * X)\n",
    "\n",
    "# def sinc_conv(X, num_filters, kernel_size, dilation_rate, name):\n",
    "#     sinc_init = np.zeros((kernel_size, 1, num_filters))\n",
    "#     for i in range(num_filters):\n",
    "#         sinc_init[:, 0, i] = sinc(np.linspace(-kernel_size//2 + 1, kernel_size//2 + 1, kernel_size), dilation_rate * (i + 1))\n",
    "#     sinc_init = sinc_init / np.linalg.norm(sinc_init, axis=(0, 1))\n",
    "#     return Conv1D(num_filters, kernel_size, dilation_rate=dilation_rate, activation='relu', kernel_initializer=keras.initializers.Constant(value=sinc_init), name=name)(X)\n",
    "\n",
    "def sinc_conv(X, num_filters, kernel_size, dilation_rate, name):\n",
    "    sinc_init = np.zeros((kernel_size, 1, num_filters))\n",
    "    for i in range(num_filters):\n",
    "        sinc_init[:, 0, i] = sinc(np.linspace(-kernel_size//2 + 1, kernel_size//2 + 1, kernel_size), dilation_rate * (i + 1))\n",
    "    sinc_init = sinc_init / np.linalg.norm(sinc_init, axis=(0, 1))\n",
    "    sinc_init = sinc_init.reshape((kernel_size, num_filters))\n",
    "    return Conv1D(num_filters, kernel_size, dilation_rate=dilation_rate, activation='relu', kernel_initializer=keras.initializers.Constant(value=sinc_init), name=name)(X)\n",
    "\n",
    "\n",
    "def build_sincnet():\n",
    "    inputs = Input(shape=(300, 1))\n",
    "    X = sinc_conv(inputs, 32, 7, 1, 'sinc_conv1')\n",
    "    X = MaxPooling1D(pool_size=3, strides=3, padding='same')(X)\n",
    "    X = sinc_conv(X, 64, 7, 2, 'sinc_conv2')\n",
    "    X = MaxPooling1D(pool_size=3, strides=3, padding='same')(X)\n",
    "    X = sinc_conv(X, 128, 7, 4, 'sinc_conv3')\n",
    "    X = MaxPooling1D(pool_size=3, strides=3, padding='same')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    outputs = Dense(2, activation='sigmoid')(X)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_sincnet()\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
